{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "asbab_nuzul = pd.read_excel('asbabun_nuzul.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(\"\\n\\n\", \" \", text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text cleaning\n",
    "asbab_nuzul['clean'] = [clean_text(i) for i in asbab_nuzul.nuzul]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\AINA\n",
      "[nltk_data]     REDIZO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Malay stopwords from NLTK\n",
    "stopword_malay = stopwords.words(\"indonesian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom stopwords from file\n",
    "txt_stopword = pd.read_csv(\"stopword-list.txt\", names=[\"stopwords\"], header=None)\n",
    "custom_stopwords = set(txt_stopword[\"stopwords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both sets of stopwords\n",
    "all_stopwords = set(stopword_malay).union(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in all_stopwords]\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stopword removal\n",
    "asbab_nuzul['Stopwords'] = asbab_nuzul['clean'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BERTopic model and fit-transform documents\n",
    "docs = asbab_nuzul['Stopwords'].tolist()\n",
    "topic_model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\")\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 11:27:52,618 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "# Save the BERTopic model\n",
    "topic_model.save(\"bertopic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the topics and documents to a CSV file\n",
    "df = pd.DataFrame({\"topic\": topics, \"document\": docs, \"surah\": asbab_nuzul['surah']})\n",
    "df.to_csv('bertopic_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all documents\n",
    "document_vectors = model.encode(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the vectors and corresponding metadata\n",
    "vector_df = pd.DataFrame(document_vectors)\n",
    "vector_df['topic'] = topics\n",
    "vector_df['document'] = docs\n",
    "vector_df['surah'] = asbab_nuzul['surah']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vector DataFrame to a CSV file\n",
    "vector_df.to_csv('document_vectors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform k-NN search\n",
    "def knn_search(input_keyword, model, document_vectors, docs, surahs, k=2):\n",
    "    # Encode the input keyword\n",
    "    vector_of_input_keyword = model.encode(input_keyword)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity([vector_of_input_keyword], document_vectors)[0]\n",
    "    \n",
    "    # Get the indices of the top k most similar documents\n",
    "    top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "    \n",
    "    # Retrieve the top k most similar documents and their surahs\n",
    "    top_k_docs = [(docs[idx], surahs[idx]) for idx in top_k_indices]\n",
    "    return top_k_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ibnu hibban shahihnya ibnu mardawaih ibnu umar. berkata. turun 261. Rasulullah berdoa.\"ya Allah. berilah tambahn ummatku\\' . turunlah', 2), ('Imam Al-Hakim Aisyah berkata. \"Ketika turun ayat. \\'Wahai berselimut (Muhammad)! Bangunlah (untuk shalat) malam hari. kecuali kecil.\\' (Nabi saw sahabat) shalat malam henti kaki-kaki bengkak. Allah menurunkan 20 surah Al-Muzzammil. \\'...karena baclah (bagimu) Al-Qur\\'an....\\' \" Ibnu Jarir hadits diatas Ibnu Abbas lainnya.', 73)]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_keyword = \"quran\"\n",
    "top_k_docs = knn_search(input_keyword, model, document_vectors, docs, asbab_nuzul['surah'].tolist(), k=2)\n",
    "print(top_k_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
